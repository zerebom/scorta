{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split,KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"001_tutorial\"\n",
    "input_dir = \"../input\"\n",
    "output_dir = \"../output\"\n",
    "\n",
    "exp_dir = f\"{output_dir}/{exp_name}\"\n",
    "feature_dir = f\"{input_dir}/feature\"\n",
    "\n",
    "Path(feature_dir).mkdir(exist_ok=True, parents=True)\n",
    "Path(exp_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# %%\n",
    "raw_train = pl.read_csv(f\"{input_dir}/train.csv\")\n",
    "raw_test = pl.read_csv(f\"{input_dir}/test.csv\")\n",
    "anime = pl.read_csv(f\"{input_dir}/anime.csv\")\n",
    "sub = pl.read_csv(f\"{input_dir}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_anime(df, anime_df):\n",
    "    df = df.join(anime_df, how=\"left\", on=\"anime_id\")\n",
    "\n",
    "    if \"row_nr\" not in df.columns:\n",
    "        df = df.with_row_count()\n",
    "    return df\n",
    "\n",
    "raw_X_test = join_anime(raw_test, anime)\n",
    "raw_train = join_anime(raw_train, anime)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-08 18:09:12\u001b[0m | INFO | \u001b[34mlogger.py:wrapper:49\u001b[0m | \u001b[1m[PID:75885] Starting fit() at ../../../../../../../../../var/folders/z0/sjx59b5j2ql1n4gd96t8rlg40000gq/T/ipykernel_75885/481370097.py:20\u001b[0m\n",
      "\u001b[32m2023-12-08 18:09:12\u001b[0m | INFO | \u001b[34mlogger.py:wrapper:55\u001b[0m | \u001b[1m[PID:75885] Finished fit() in 0.1354 seconds.\n",
      "\u001b[0m\n",
      "\u001b[32m2023-12-08 18:09:12\u001b[0m | INFO | \u001b[34mlogger.py:wrapper:49\u001b[0m | \u001b[1m[PID:75885] Starting fit() at ../../../../../../../../../var/folders/z0/sjx59b5j2ql1n4gd96t8rlg40000gq/T/ipykernel_75885/481370097.py:20\u001b[0m\n",
      "\u001b[32m2023-12-08 18:09:12\u001b[0m | INFO | \u001b[34mlogger.py:wrapper:55\u001b[0m | \u001b[1m[PID:75885] Finished fit() in 0.0112 seconds.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scorta.utils.logger import timing_logger\n",
    "from abc import ABC\n",
    "\n",
    "class PlFeature(ABC):\n",
    "    def __init__(self, feature_dir:Path|str,suffix:str=\"\"):\n",
    "        if self.__class__.__name__.isupper():\n",
    "            self.name = self.__class__.__name__.lower()\n",
    "        else:\n",
    "            self.name = re.sub(\"([A-Z])\", lambda x: \"_\" + x.group(1).lower(), self.__class__.__name__).lstrip(\"_\")\n",
    "\n",
    "        self.name = self.__class__.__name__\n",
    "        self.feature_dir = Path(feature_dir)\n",
    "        self.feature_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.feature_path = self.feature_dir / f\"{self.name}_{suffix}.parquet\"\n",
    "\n",
    "    def fit(self)->pl.DataFrame:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_feautre(self):\n",
    "        df = self.fit()\n",
    "        self.save(df)\n",
    "\n",
    "    def save(self,df:pl.DataFrame) -> None:\n",
    "        df.write_parquet(self.feature_path)\n",
    "\n",
    "    def load(self) -> pl.DataFrame:\n",
    "        return pl.read_parquet(self.feature_path)\n",
    "\n",
    "class MemberRatio(PlFeature):\n",
    "    def __init__(self,df:pl.DataFrame, feature_dir:str,suffix:str):\n",
    "        super().__init__(feature_dir,suffix)\n",
    "        self.df = df\n",
    "        self.key_cols = [\"user_id\",\"anime_id\"]\n",
    "        self.feature_cols = [\"watching_rate\",\"completed_rate\",\"on_hold_rate\",\"dropped_rate\"]\n",
    "\n",
    "    @timing_logger\n",
    "    def fit(self) -> pl.DataFrame:\n",
    "        df = self.df.with_columns(\n",
    "            [\n",
    "                (pl.col(\"watching\") / pl.col(\"members\")).alias(\"watching_rate\"),\n",
    "                (pl.col(\"completed\") / pl.col(\"members\")).alias(\"completed_rate\"),\n",
    "                (pl.col(\"on_hold\") / pl.col(\"members\")).alias(\"on_hold_rate\"),\n",
    "                (pl.col(\"dropped\") / pl.col(\"members\")).alias(\"dropped_rate\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return df[self.key_cols + self.feature_cols]\n",
    "\n",
    "\n",
    "feats = [MemberRatio(raw_train,f\"{input_dir}/feature\",\"train\"),MemberRatio(raw_X_test,f\"{input_dir}/feature\",\"test\")]\n",
    "\n",
    "for feat in feats:\n",
    "    feat.create_feautre()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureMerger():\n",
    "    def __init__(self,features:list[PlFeature])-> None:\n",
    "       self.features = features\n",
    "\n",
    "    def merge(self,df:pl.DataFrame)-> pl.DataFrame:\n",
    "        for feature in self.features:\n",
    "            feat_df = feature.load()\n",
    "            df = df.join(feat_df,how=\"left\",on=feature.key_cols)\n",
    "        return df\n",
    "\n",
    "tr_fm = FeatureMerger([MemberRatio(raw_train, feature_dir, \"train\")])\n",
    "te_fm = FeatureMerger([MemberRatio(raw_X_test, feature_dir, \"test\")])\n",
    "\n",
    "X_train = tr_fm.merge(raw_train)\n",
    "X_test = te_fm.merge(raw_X_test).select(~cs.string())\n",
    "\n",
    "import polars.selectors as cs\n",
    "y_train = X_train[\"score\"].to_numpy()\n",
    "X_train = X_train.select(~cs.string()).drop(\"score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2777\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 11\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Start training from score 7.768759\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's l2: 1.97115\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2777\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 11\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Start training from score 7.768725\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid_0's l2: 1.97189\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2777\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 11\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Start training from score 7.768770\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 1.96933\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2776\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 11\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Start training from score 7.768798\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's l2: 1.99474\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2777\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 11\n",
      "[LightGBM] [Warning] early_stopping_round is set=1, early_stopping_rounds=1 will be ignored. Current value: early_stopping_round=1\n",
      "[LightGBM] [Info] Start training from score 7.768798\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's l2: 2.00047\n"
     ]
    }
   ],
   "source": [
    "from scorta.model.gradient_boost import GBTWrapper\n",
    "gbdt = GBTWrapper(\"lgb\",\"reg\")\n",
    "models, oof = gbdt.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([gbdt.predict(X_test,i) for i in range(5)]).mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scorta-oturhGc--py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
